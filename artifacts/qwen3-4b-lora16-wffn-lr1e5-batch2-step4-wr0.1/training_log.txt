===== Training started at 0 =====
Batch size: 2, LR: 1e-05, Epochs: 10
Steps per file: 50, Grad. accum: 4, Warmup rate: 0.1
Validation dataset loaded with 128 steps, using seed 42, max files 128, steps/file 1, batch size 8
Total validation batches: 16
Warmup steps: 3750, Total optimizer steps: 37500
[Epoch 1] step 0 loss 1.1965 lr 0.000000
[Epoch 1] step 100 loss 0.5275 lr 0.000000
[Epoch 1] step 200 loss 0.2345 lr 0.000000
[Epoch 1] step 300 loss 0.2926 lr 0.000000
[Epoch 1] step 400 loss 0.3564 lr 0.000000
[Epoch 1] step 500 loss 0.4167 lr 0.000000
[Epoch 1] step 600 loss 0.6093 lr 0.000000
[Epoch 1] step 700 loss 0.2473 lr 0.000000
[Epoch 1] step 800 loss 0.2599 lr 0.000001
[Epoch 1] step 900 loss 0.1408 lr 0.000001
[Epoch 1] step 1000 loss 0.2600 lr 0.000001
[Epoch 1] step 1100 loss 0.1164 lr 0.000001
[Epoch 1] step 1200 loss 0.1232 lr 0.000001
[Epoch 1] step 1300 loss 0.1218 lr 0.000001
[Epoch 1] step 1400 loss 0.1972 lr 0.000001
[Epoch 1] step 1500 loss 0.0696 lr 0.000001
[Epoch 1] step 1600 loss 0.1853 lr 0.000001
[Epoch 1] step 1700 loss 0.1431 lr 0.000001
[Epoch 1] step 1800 loss 0.0946 lr 0.000001
[Epoch 1] step 1900 loss 0.1063 lr 0.000001
[Epoch 1] step 2000 loss 0.0860 lr 0.000001
[Epoch 1] step 2100 loss 0.1197 lr 0.000001
[Epoch 1] step 2200 loss 0.0566 lr 0.000001
[Epoch 1] step 2300 loss 0.0669 lr 0.000002
[Epoch 1] step 2400 loss 0.1672 lr 0.000002
[Epoch 1] step 2500 loss 0.0444 lr 0.000002
[Epoch 1] step 2600 loss 0.1180 lr 0.000002
[Epoch 1] step 2700 loss 0.1053 lr 0.000002
[Epoch 1] step 2800 loss 0.1818 lr 0.000002
[Epoch 1] step 2900 loss 0.1775 lr 0.000002
[Epoch 1] step 3000 loss 0.0575 lr 0.000002
[Epoch 1] step 3100 loss 0.1129 lr 0.000002
[Epoch 1] step 3200 loss 0.0551 lr 0.000002
[Epoch 1] step 3300 loss 0.2785 lr 0.000002
[Epoch 1] step 3400 loss 0.1039 lr 0.000002
[Epoch 1] step 3500 loss 0.2011 lr 0.000002
[Epoch 1] step 3600 loss 0.0555 lr 0.000002
[Epoch 1] step 3700 loss 0.0766 lr 0.000002
[Epoch 1] step 3800 loss 0.0844 lr 0.000003
[Epoch 1] step 3900 loss 0.1156 lr 0.000003
[Epoch 1] step 4000 loss 0.1085 lr 0.000003
[Epoch 1] step 4100 loss 0.1442 lr 0.000003
[Epoch 1] step 4200 loss 0.0374 lr 0.000003
[Epoch 1] step 4300 loss 0.0519 lr 0.000003
[Epoch 1] step 4400 loss 0.0164 lr 0.000003
[Epoch 1] step 4500 loss 0.0731 lr 0.000003
[Epoch 1] step 4600 loss 0.0272 lr 0.000003
[Epoch 1] step 4700 loss 0.1743 lr 0.000003
[Epoch 1] step 4800 loss 0.0985 lr 0.000003
[Epoch 1] step 4900 loss 0.0430 lr 0.000003
[Validation] global_step 5000 loss 0.2830 accuracy 0.4023
Validation accuracy improved from 0.0000 to 0.4023, Validation loss improved from inf to 0.2830
[Epoch 1] step 5000 loss 0.3122 lr 0.000003
[Epoch 1] step 5100 loss 0.0510 lr 0.000003
[Epoch 1] step 5200 loss 0.0313 lr 0.000003
[Epoch 1] step 5300 loss 0.1217 lr 0.000004
[Epoch 1] step 5400 loss 0.0176 lr 0.000004
[Epoch 1] step 5500 loss 0.0524 lr 0.000004
[Epoch 1] step 5600 loss 0.0429 lr 0.000004
[Epoch 1] step 5700 loss 0.0349 lr 0.000004
[Epoch 1] step 5800 loss 0.0597 lr 0.000004
[Epoch 1] step 5900 loss 0.0889 lr 0.000004
[Epoch 1] step 6000 loss 0.1013 lr 0.000004
[Epoch 1] step 6100 loss 0.1284 lr 0.000004
[Epoch 1] step 6200 loss 0.0534 lr 0.000004
[Epoch 1] step 6300 loss 0.0613 lr 0.000004
[Epoch 1] step 6400 loss 0.0625 lr 0.000004
[Epoch 1] step 6500 loss 0.0214 lr 0.000004
[Epoch 1] step 6600 loss 0.0359 lr 0.000004
[Epoch 1] step 6700 loss 0.0562 lr 0.000004
[Epoch 1] step 6800 loss 0.0398 lr 0.000005
[Epoch 1] step 6900 loss 0.0629 lr 0.000005
[Epoch 1] step 7000 loss 0.0079 lr 0.000005
[Epoch 1] step 7100 loss 0.0028 lr 0.000005
[Epoch 1] step 7200 loss 0.0155 lr 0.000005
[Epoch 1] step 7300 loss 0.0485 lr 0.000005
[Epoch 1] step 7400 loss 0.0664 lr 0.000005
[Epoch 1] step 7500 loss 0.0651 lr 0.000005
[Epoch 1] step 7600 loss 0.0972 lr 0.000005
[Epoch 1] step 7700 loss 0.0548 lr 0.000005
[Epoch 1] step 7800 loss 0.1493 lr 0.000005
[Epoch 1] step 7900 loss 0.0627 lr 0.000005
[Epoch 1] step 8000 loss 0.0405 lr 0.000005
[Epoch 1] step 8100 loss 0.0908 lr 0.000005
[Epoch 1] step 8200 loss 0.0668 lr 0.000005
[Epoch 1] step 8300 loss 0.0760 lr 0.000006
[Epoch 1] step 8400 loss 0.0448 lr 0.000006
[Epoch 1] step 8500 loss 0.0034 lr 0.000006
[Epoch 1] step 8600 loss 0.0210 lr 0.000006
[Epoch 1] step 8700 loss 0.0687 lr 0.000006
[Epoch 1] step 8800 loss 0.0060 lr 0.000006
[Epoch 1] step 8900 loss 0.0238 lr 0.000006
[Epoch 1] step 9000 loss 0.0638 lr 0.000006
[Epoch 1] step 9100 loss 0.0559 lr 0.000006
[Epoch 1] step 9200 loss 0.0499 lr 0.000006
[Epoch 1] step 9300 loss 0.1485 lr 0.000006
[Epoch 1] step 9400 loss 0.1454 lr 0.000006
[Epoch 1] step 9500 loss 0.2241 lr 0.000006
[Epoch 1] step 9600 loss 0.1103 lr 0.000006
[Epoch 1] step 9700 loss 0.0651 lr 0.000006
[Epoch 1] step 9800 loss 0.0966 lr 0.000007
[Epoch 1] step 9900 loss 0.0707 lr 0.000007
[Validation] global_step 10000 loss 0.2052 accuracy 0.4492
Validation accuracy improved from 0.4023 to 0.4492, Validation loss improved from 0.2830 to 0.2052
[Epoch 1] step 10000 loss 0.0279 lr 0.000007
[Epoch 1] step 10100 loss 0.0122 lr 0.000007
[Epoch 1] step 10200 loss 0.1583 lr 0.000007
[Epoch 1] step 10300 loss 0.0360 lr 0.000007
[Epoch 1] step 10400 loss 0.0389 lr 0.000007
[Epoch 1] step 10500 loss 0.0095 lr 0.000007
[Epoch 1] step 10600 loss 0.0382 lr 0.000007
[Epoch 1] step 10700 loss 0.0348 lr 0.000007
[Epoch 1] step 10800 loss 0.1014 lr 0.000007
[Epoch 1] step 10900 loss 0.0315 lr 0.000007
[Epoch 1] step 11000 loss 0.0732 lr 0.000007
[Epoch 1] step 11100 loss 0.0266 lr 0.000007
[Epoch 1] step 11200 loss 0.0420 lr 0.000007
[Epoch 1] step 11300 loss 0.0427 lr 0.000008
[Epoch 1] step 11400 loss 0.0673 lr 0.000008
[Epoch 1] step 11500 loss 0.0669 lr 0.000008
[Epoch 1] step 11600 loss 0.0398 lr 0.000008
[Epoch 1] step 11700 loss 0.1022 lr 0.000008
[Epoch 1] step 11800 loss 0.0212 lr 0.000008
[Epoch 1] step 11900 loss 0.0584 lr 0.000008
[Epoch 1] step 12000 loss 0.0710 lr 0.000008
[Epoch 1] step 12100 loss 0.0156 lr 0.000008
[Epoch 1] step 12200 loss 0.0427 lr 0.000008
[Epoch 1] step 12300 loss 0.0308 lr 0.000008
[Epoch 1] step 12400 loss 0.0652 lr 0.000008
[Epoch 1] step 12500 loss 0.0155 lr 0.000008
[Epoch 1] step 12600 loss 0.0257 lr 0.000008
[Epoch 1] step 12700 loss 0.0369 lr 0.000008
[Epoch 1] step 12800 loss 0.0246 lr 0.000009
[Epoch 1] step 12900 loss 0.1172 lr 0.000009
[Epoch 1] step 13000 loss 0.0224 lr 0.000009
[Epoch 1] step 13100 loss 0.0528 lr 0.000009
[Epoch 1] step 13200 loss 0.0526 lr 0.000009
[Epoch 1] step 13300 loss 0.0487 lr 0.000009
[Epoch 1] step 13400 loss 0.0603 lr 0.000009
[Epoch 1] step 13500 loss 0.0817 lr 0.000009
[Epoch 1] step 13600 loss 0.0109 lr 0.000009
[Epoch 1] step 13700 loss 0.0902 lr 0.000009
[Epoch 1] step 13800 loss 0.0552 lr 0.000009
[Epoch 1] step 13900 loss 0.0454 lr 0.000009
[Epoch 1] step 14000 loss 0.0165 lr 0.000009
[Epoch 1] step 14100 loss 0.0147 lr 0.000009
[Epoch 1] step 14200 loss 0.0352 lr 0.000009
[Epoch 1] step 14300 loss 0.0533 lr 0.000010
[Epoch 1] step 14400 loss 0.0477 lr 0.000010
[Epoch 1] step 14500 loss 0.0478 lr 0.000010
[Epoch 1] step 14600 loss 0.0112 lr 0.000010
[Epoch 1] step 14700 loss 0.0389 lr 0.000010
[Epoch 1] step 14800 loss 0.0228 lr 0.000010
[Epoch 1] step 14900 loss 0.0361 lr 0.000010
[Validation] global_step 15000 loss 0.1904 accuracy 0.4453
Validation loss improved from 0.2052 to 0.1904
Epoch 1 avg loss 0.0983
[Epoch 2] step 0 loss 0.0302 lr 0.000010
[Epoch 2] step 100 loss 0.0150 lr 0.000010
[Epoch 2] step 200 loss 0.0230 lr 0.000010
[Epoch 2] step 300 loss 0.0357 lr 0.000010
[Epoch 2] step 400 loss 0.0215 lr 0.000010
[Epoch 2] step 500 loss 0.0912 lr 0.000010
[Epoch 2] step 600 loss 0.0942 lr 0.000010
[Epoch 2] step 700 loss 0.0586 lr 0.000010
[Epoch 2] step 800 loss 0.0304 lr 0.000010
[Epoch 2] step 900 loss 0.0922 lr 0.000010
[Epoch 2] step 1000 loss 0.0133 lr 0.000010
[Epoch 2] step 1100 loss 0.0529 lr 0.000010
[Epoch 2] step 1200 loss 0.0791 lr 0.000010
[Epoch 2] step 1300 loss 0.0840 lr 0.000010
[Epoch 2] step 1400 loss 0.0317 lr 0.000010
[Epoch 2] step 1500 loss 0.0163 lr 0.000010
[Epoch 2] step 1600 loss 0.0471 lr 0.000010
[Epoch 2] step 1700 loss 0.0204 lr 0.000010
[Epoch 2] step 1800 loss 0.0498 lr 0.000010
[Epoch 2] step 1900 loss 0.0370 lr 0.000010
[Epoch 2] step 2000 loss 0.0367 lr 0.000010
[Epoch 2] step 2100 loss 0.0458 lr 0.000010
[Epoch 2] step 2200 loss 0.0315 lr 0.000010
[Epoch 2] step 2300 loss 0.0511 lr 0.000010
[Epoch 2] step 2400 loss 0.0702 lr 0.000010
[Epoch 2] step 2500 loss 0.0466 lr 0.000010
[Epoch 2] step 2600 loss 0.0239 lr 0.000010
[Epoch 2] step 2700 loss 0.0425 lr 0.000010
[Epoch 2] step 2800 loss 0.0548 lr 0.000010
[Epoch 2] step 2900 loss 0.0278 lr 0.000010
[Epoch 2] step 3000 loss 0.1097 lr 0.000010
[Epoch 2] step 3100 loss 0.0658 lr 0.000010
[Epoch 2] step 3200 loss 0.0312 lr 0.000010
[Epoch 2] step 3300 loss 0.0231 lr 0.000010
[Epoch 2] step 3400 loss 0.0158 lr 0.000010
[Epoch 2] step 3500 loss 0.0302 lr 0.000010
[Epoch 2] step 3600 loss 0.1013 lr 0.000010
[Epoch 2] step 3700 loss 0.1596 lr 0.000010
[Epoch 2] step 3800 loss 0.0300 lr 0.000010
[Epoch 2] step 3900 loss 0.1469 lr 0.000010
[Epoch 2] step 4000 loss 0.0287 lr 0.000010
[Epoch 2] step 4100 loss 0.0059 lr 0.000010
[Epoch 2] step 4200 loss 0.0384 lr 0.000010
[Epoch 2] step 4300 loss 0.0384 lr 0.000010
[Epoch 2] step 4400 loss 0.0552 lr 0.000010
[Epoch 2] step 4500 loss 0.1006 lr 0.000010
[Epoch 2] step 4600 loss 0.0777 lr 0.000010
[Epoch 2] step 4700 loss 0.0218 lr 0.000010
[Epoch 2] step 4800 loss 0.0411 lr 0.000010
[Epoch 2] step 4900 loss 0.0273 lr 0.000010
[Validation] global_step 20000 loss 0.1867 accuracy 0.4688
Validation accuracy improved from 0.4492 to 0.4688, Validation loss improved from 0.1904 to 0.1867
[Epoch 2] step 5000 loss 0.0141 lr 0.000010
[Epoch 2] step 5100 loss 0.0213 lr 0.000010
[Epoch 2] step 5200 loss 0.0374 lr 0.000010
[Epoch 2] step 5300 loss 0.1088 lr 0.000010
[Epoch 2] step 5400 loss 0.0253 lr 0.000010
[Epoch 2] step 5500 loss 0.0461 lr 0.000010
[Epoch 2] step 5600 loss 0.0620 lr 0.000010
[Epoch 2] step 5700 loss 0.0540 lr 0.000010
[Epoch 2] step 5800 loss 0.2599 lr 0.000010
[Epoch 2] step 5900 loss 0.0674 lr 0.000010
[Epoch 2] step 6000 loss 0.0545 lr 0.000010
[Epoch 2] step 6100 loss 0.0581 lr 0.000010
[Epoch 2] step 6200 loss 0.0290 lr 0.000010
[Epoch 2] step 6300 loss 0.0398 lr 0.000010
[Epoch 2] step 6400 loss 0.0708 lr 0.000010
[Epoch 2] step 6500 loss 0.0237 lr 0.000010
[Epoch 2] step 6600 loss 0.0405 lr 0.000010
[Epoch 2] step 6700 loss 0.0175 lr 0.000010
[Epoch 2] step 6800 loss 0.0130 lr 0.000009
[Epoch 2] step 6900 loss 0.0212 lr 0.000009
[Epoch 2] step 7000 loss 0.0353 lr 0.000009
[Epoch 2] step 7100 loss 0.0507 lr 0.000009
[Epoch 2] step 7200 loss 0.0722 lr 0.000009
[Epoch 2] step 7300 loss 0.0253 lr 0.000009
[Epoch 2] step 7400 loss 0.1080 lr 0.000009
[Epoch 2] step 7500 loss 0.0584 lr 0.000009
[Epoch 2] step 7600 loss 0.0345 lr 0.000009
[Epoch 2] step 7700 loss 0.0505 lr 0.000009
[Epoch 2] step 7800 loss 0.0099 lr 0.000009
[Epoch 2] step 7900 loss 0.0281 lr 0.000009
[Epoch 2] step 8000 loss 0.0791 lr 0.000009
[Epoch 2] step 8100 loss 0.0725 lr 0.000009
[Epoch 2] step 8200 loss 0.0697 lr 0.000009
[Epoch 2] step 8300 loss 0.0418 lr 0.000009
[Epoch 2] step 8400 loss 0.0334 lr 0.000009
[Epoch 2] step 8500 loss 0.0587 lr 0.000009
[Epoch 2] step 8600 loss 0.0298 lr 0.000009
[Epoch 2] step 8700 loss 0.0825 lr 0.000009
[Epoch 2] step 8800 loss 0.0500 lr 0.000009
[Epoch 2] step 8900 loss 0.0266 lr 0.000009
[Epoch 2] step 9000 loss 0.0430 lr 0.000009
[Epoch 2] step 9100 loss 0.0199 lr 0.000009
[Epoch 2] step 9200 loss 0.0893 lr 0.000009
[Epoch 2] step 9300 loss 0.0500 lr 0.000009
[Epoch 2] step 9400 loss 0.1081 lr 0.000009
[Epoch 2] step 9500 loss 0.0217 lr 0.000009
[Epoch 2] step 9600 loss 0.0676 lr 0.000009
[Epoch 2] step 9700 loss 0.2769 lr 0.000009
[Epoch 2] step 9800 loss 0.0417 lr 0.000009
[Epoch 2] step 9900 loss 0.0189 lr 0.000009
[Validation] global_step 25000 loss 0.1923 accuracy 0.4766
Validation accuracy improved from 0.4688 to 0.4766
[Epoch 2] step 10000 loss 0.0719 lr 0.000009
[Epoch 2] step 10100 loss 0.0262 lr 0.000009
[Epoch 2] step 10200 loss 0.0240 lr 0.000009
[Epoch 2] step 10300 loss 0.1268 lr 0.000009
[Epoch 2] step 10400 loss 0.0116 lr 0.000009
[Epoch 2] step 10500 loss 0.1465 lr 0.000009
[Epoch 2] step 10600 loss 0.0337 lr 0.000009
[Epoch 2] step 10700 loss 0.0380 lr 0.000009
[Epoch 2] step 10800 loss 0.0287 lr 0.000009
[Epoch 2] step 10900 loss 0.0545 lr 0.000009
[Epoch 2] step 11000 loss 0.2068 lr 0.000009
[Epoch 2] step 11100 loss 0.0589 lr 0.000009
[Epoch 2] step 11200 loss 0.1974 lr 0.000009
[Epoch 2] step 11300 loss 0.1252 lr 0.000009
[Epoch 2] step 11400 loss 0.0836 lr 0.000009
[Epoch 2] step 11500 loss 0.1200 lr 0.000009
[Epoch 2] step 11600 loss 0.0268 lr 0.000009
[Epoch 2] step 11700 loss 0.0781 lr 0.000009
[Epoch 2] step 11800 loss 0.0807 lr 0.000009
[Epoch 2] step 11900 loss 0.0146 lr 0.000009
[Epoch 2] step 12000 loss 0.0470 lr 0.000009
[Epoch 2] step 12100 loss 0.1100 lr 0.000009
[Epoch 2] step 12200 loss 0.0043 lr 0.000009
[Epoch 2] step 12300 loss 0.0555 lr 0.000009
[Epoch 2] step 12400 loss 0.0228 lr 0.000009
[Epoch 2] step 12500 loss 0.0281 lr 0.000009
[Epoch 2] step 12600 loss 0.0781 lr 0.000009
[Epoch 2] step 12700 loss 0.0779 lr 0.000009
[Epoch 2] step 12800 loss 0.0485 lr 0.000009
[Epoch 2] step 12900 loss 0.0906 lr 0.000009
[Epoch 2] step 13000 loss 0.0263 lr 0.000009
[Epoch 2] step 13100 loss 0.0568 lr 0.000009
[Epoch 2] step 13200 loss 0.1094 lr 0.000009
[Epoch 2] step 13300 loss 0.0538 lr 0.000009
[Epoch 2] step 13400 loss 0.0114 lr 0.000009
[Epoch 2] step 13500 loss 0.0456 lr 0.000009
[Epoch 2] step 13600 loss 0.0061 lr 0.000009
[Epoch 2] step 13700 loss 0.0114 lr 0.000009
[Epoch 2] step 13800 loss 0.0943 lr 0.000009
[Epoch 2] step 13900 loss 0.0421 lr 0.000009
[Epoch 2] step 14000 loss 0.0628 lr 0.000009
[Epoch 2] step 14100 loss 0.0677 lr 0.000009
[Epoch 2] step 14200 loss 0.0397 lr 0.000009
[Epoch 2] step 14300 loss 0.0249 lr 0.000009
[Epoch 2] step 14400 loss 0.0748 lr 0.000009
[Epoch 2] step 14500 loss 0.0531 lr 0.000009
[Epoch 2] step 14600 loss 0.0523 lr 0.000009
[Epoch 2] step 14700 loss 0.0155 lr 0.000009
[Epoch 2] step 14800 loss 0.0246 lr 0.000009
[Epoch 2] step 14900 loss 0.0553 lr 0.000009
[Validation] global_step 30000 loss 0.1976 accuracy 0.4844
Validation accuracy improved from 0.4766 to 0.4844
Epoch 2 avg loss 0.0556
[Epoch 3] step 0 loss 0.1752 lr 0.000009
[Epoch 3] step 100 loss 0.0583 lr 0.000009
[Epoch 3] step 200 loss 0.1126 lr 0.000009
[Epoch 3] step 300 loss 0.0181 lr 0.000009
[Epoch 3] step 400 loss 0.0230 lr 0.000009
[Epoch 3] step 500 loss 0.0968 lr 0.000009
[Epoch 3] step 600 loss 0.0162 lr 0.000009
[Epoch 3] step 700 loss 0.0200 lr 0.000009
[Epoch 3] step 800 loss 0.0313 lr 0.000009
[Epoch 3] step 900 loss 0.0802 lr 0.000009
[Epoch 3] step 1000 loss 0.0300 lr 0.000009
[Epoch 3] step 1100 loss 0.0656 lr 0.000009
[Epoch 3] step 1200 loss 0.0911 lr 0.000009
[Epoch 3] step 1300 loss 0.0082 lr 0.000009
[Epoch 3] step 1400 loss 0.0756 lr 0.000009
[Epoch 3] step 1500 loss 0.0104 lr 0.000009
[Epoch 3] step 1600 loss 0.2981 lr 0.000009
[Epoch 3] step 1700 loss 0.0334 lr 0.000009
[Epoch 3] step 1800 loss 0.1002 lr 0.000009
[Epoch 3] step 1900 loss 0.1162 lr 0.000009
[Epoch 3] step 2000 loss 0.1241 lr 0.000009
[Epoch 3] step 2100 loss 0.0184 lr 0.000009
[Epoch 3] step 2200 loss 0.0290 lr 0.000009
[Epoch 3] step 2300 loss 0.0970 lr 0.000009
[Epoch 3] step 2400 loss 0.0924 lr 0.000009
[Epoch 3] step 2500 loss 0.0565 lr 0.000009
[Epoch 3] step 2600 loss 0.0741 lr 0.000009
[Epoch 3] step 2700 loss 0.0159 lr 0.000009
[Epoch 3] step 2800 loss 0.0334 lr 0.000009
[Epoch 3] step 2900 loss 0.1460 lr 0.000009
[Epoch 3] step 3000 loss 0.1105 lr 0.000009
[Epoch 3] step 3100 loss 0.0518 lr 0.000009
[Epoch 3] step 3200 loss 0.0175 lr 0.000009
[Epoch 3] step 3300 loss 0.0240 lr 0.000009
[Epoch 3] step 3400 loss 0.0088 lr 0.000009
[Epoch 3] step 3500 loss 0.0289 lr 0.000009
[Epoch 3] step 3600 loss 0.0364 lr 0.000009
[Epoch 3] step 3700 loss 0.0291 lr 0.000009
[Epoch 3] step 3800 loss 0.0561 lr 0.000009
[Epoch 3] step 3900 loss 0.0559 lr 0.000009
[Epoch 3] step 4000 loss 0.0474 lr 0.000009
[Epoch 3] step 4100 loss 0.0367 lr 0.000009
[Epoch 3] step 4200 loss 0.1416 lr 0.000009
[Epoch 3] step 4300 loss 0.0550 lr 0.000009
[Epoch 3] step 4400 loss 0.0720 lr 0.000009
[Epoch 3] step 4500 loss 0.0560 lr 0.000009
[Epoch 3] step 4600 loss 0.0074 lr 0.000009
[Epoch 3] step 4700 loss 0.0571 lr 0.000009
[Epoch 3] step 4800 loss 0.0740 lr 0.000009
[Epoch 3] step 4900 loss 0.0179 lr 0.000009
[Validation] global_step 35000 loss 0.2103 accuracy 0.4258
Validation metrics did not improve. Patience: 1/10
[Epoch 3] step 5000 loss 0.0303 lr 0.000009
[Epoch 3] step 5100 loss 0.0260 lr 0.000009
[Epoch 3] step 5200 loss 0.0986 lr 0.000009
[Epoch 3] step 5300 loss 0.0589 lr 0.000008
[Epoch 3] step 5400 loss 0.0356 lr 0.000008
[Epoch 3] step 5500 loss 0.0291 lr 0.000008
[Epoch 3] step 5600 loss 0.0300 lr 0.000008
[Epoch 3] step 5700 loss 0.0572 lr 0.000008
[Epoch 3] step 5800 loss 0.0071 lr 0.000008
[Epoch 3] step 5900 loss 0.0666 lr 0.000008
[Epoch 3] step 6000 loss 0.0416 lr 0.000008
[Epoch 3] step 6100 loss 0.1034 lr 0.000008
[Epoch 3] step 6200 loss 0.1667 lr 0.000008
[Epoch 3] step 6300 loss 0.0198 lr 0.000008
[Epoch 3] step 6400 loss 0.0313 lr 0.000008
[Epoch 3] step 6500 loss 0.0754 lr 0.000008
[Epoch 3] step 6600 loss 0.0913 lr 0.000008
[Epoch 3] step 6700 loss 0.0296 lr 0.000008
[Epoch 3] step 6800 loss 0.1338 lr 0.000008
[Epoch 3] step 6900 loss 0.1479 lr 0.000008
[Epoch 3] step 7000 loss 0.0937 lr 0.000008
[Epoch 3] step 7100 loss 0.0226 lr 0.000008
[Epoch 3] step 7200 loss 0.0421 lr 0.000008
[Epoch 3] step 7300 loss 0.0487 lr 0.000008
[Epoch 3] step 7400 loss 0.0482 lr 0.000008
[Epoch 3] step 7500 loss 0.0301 lr 0.000008
[Epoch 3] step 7600 loss 0.0134 lr 0.000008
[Epoch 3] step 7700 loss 0.1027 lr 0.000008
[Epoch 3] step 7800 loss 0.0667 lr 0.000008
[Epoch 3] step 7900 loss 0.0660 lr 0.000008
[Epoch 3] step 8000 loss 0.0202 lr 0.000008
[Epoch 3] step 8100 loss 0.0420 lr 0.000008
[Epoch 3] step 8200 loss 0.0365 lr 0.000008
[Epoch 3] step 8300 loss 0.0472 lr 0.000008
[Epoch 3] step 8400 loss 0.1058 lr 0.000008
[Epoch 3] step 8500 loss 0.0892 lr 0.000008
[Epoch 3] step 8600 loss 0.0630 lr 0.000008
[Epoch 3] step 8700 loss 0.0356 lr 0.000008
[Epoch 3] step 8800 loss 0.0559 lr 0.000008
[Epoch 3] step 8900 loss 0.0859 lr 0.000008
[Epoch 3] step 9000 loss 0.0503 lr 0.000008
[Epoch 3] step 9100 loss 0.0095 lr 0.000008
[Epoch 3] step 9200 loss 0.1073 lr 0.000008
[Epoch 3] step 9300 loss 0.0375 lr 0.000008
[Epoch 3] step 9400 loss 0.0367 lr 0.000008
[Epoch 3] step 9500 loss 0.0509 lr 0.000008
[Epoch 3] step 9600 loss 0.0655 lr 0.000008
[Epoch 3] step 9700 loss 0.1222 lr 0.000008
[Epoch 3] step 9800 loss 0.0160 lr 0.000008
[Epoch 3] step 9900 loss 0.0319 lr 0.000008
[Validation] global_step 40000 loss 0.2249 accuracy 0.4805
Validation metrics did not improve. Patience: 2/10
[Epoch 3] step 10000 loss 0.0440 lr 0.000008
[Epoch 3] step 10100 loss 0.0233 lr 0.000008
[Epoch 3] step 10200 loss 0.0506 lr 0.000008
[Epoch 3] step 10300 loss 0.0673 lr 0.000008
[Epoch 3] step 10400 loss 0.0692 lr 0.000008
[Epoch 3] step 10500 loss 0.0503 lr 0.000008
[Epoch 3] step 10600 loss 0.0514 lr 0.000008
[Epoch 3] step 10700 loss 0.0188 lr 0.000008
[Epoch 3] step 10800 loss 0.1250 lr 0.000008
[Epoch 3] step 10900 loss 0.0405 lr 0.000008
[Epoch 3] step 11000 loss 0.0631 lr 0.000008
[Epoch 3] step 11100 loss 0.0877 lr 0.000008
[Epoch 3] step 11200 loss 0.0430 lr 0.000008
[Epoch 3] step 11300 loss 0.0705 lr 0.000008
[Epoch 3] step 11400 loss 0.0758 lr 0.000008
[Epoch 3] step 11500 loss 0.0397 lr 0.000008
[Epoch 3] step 11600 loss 0.0909 lr 0.000008
[Epoch 3] step 11700 loss 0.0117 lr 0.000008
[Epoch 3] step 11800 loss 0.0887 lr 0.000008
[Epoch 3] step 11900 loss 0.1173 lr 0.000008
[Epoch 3] step 12000 loss 0.0586 lr 0.000008
[Epoch 3] step 12100 loss 0.1391 lr 0.000008
[Epoch 3] step 12200 loss 0.0428 lr 0.000008
[Epoch 3] step 12300 loss 0.0472 lr 0.000008
[Epoch 3] step 12400 loss 0.0250 lr 0.000008
[Epoch 3] step 12500 loss 0.0146 lr 0.000008
[Epoch 3] step 12600 loss 0.0566 lr 0.000008
[Epoch 3] step 12700 loss 0.0529 lr 0.000008
[Epoch 3] step 12800 loss 0.0681 lr 0.000008
[Epoch 3] step 12900 loss 0.0747 lr 0.000008
[Epoch 3] step 13000 loss 0.0697 lr 0.000008
[Epoch 3] step 13100 loss 0.0110 lr 0.000008
[Epoch 3] step 13200 loss 0.0638 lr 0.000008
[Epoch 3] step 13300 loss 0.0423 lr 0.000008
[Epoch 3] step 13400 loss 0.0299 lr 0.000008
[Epoch 3] step 13500 loss 0.0491 lr 0.000008
[Epoch 3] step 13600 loss 0.3558 lr 0.000008
[Epoch 3] step 13700 loss 0.0102 lr 0.000008
[Epoch 3] step 13800 loss 0.0699 lr 0.000008
[Epoch 3] step 13900 loss 0.0381 lr 0.000008
[Epoch 3] step 14000 loss 0.0691 lr 0.000008
[Epoch 3] step 14100 loss 0.0672 lr 0.000008
[Epoch 3] step 14200 loss 0.0149 lr 0.000008
[Epoch 3] step 14300 loss 0.0687 lr 0.000008
[Epoch 3] step 14400 loss 0.0585 lr 0.000008
[Epoch 3] step 14500 loss 0.0607 lr 0.000008
[Epoch 3] step 14600 loss 0.0978 lr 0.000008
[Epoch 3] step 14700 loss 0.0492 lr 0.000008
[Epoch 3] step 14800 loss 0.0489 lr 0.000008
[Epoch 3] step 14900 loss 0.0562 lr 0.000008
[Validation] global_step 45000 loss 0.2459 accuracy 0.4727
Validation metrics did not improve. Patience: 3/10
Epoch 3 avg loss 0.0603
[Epoch 4] step 0 loss 0.0411 lr 0.000008
[Epoch 4] step 100 loss 0.0684 lr 0.000008
[Epoch 4] step 200 loss 0.0230 lr 0.000008
[Epoch 4] step 300 loss 0.0910 lr 0.000008
[Epoch 4] step 400 loss 0.0719 lr 0.000008
[Epoch 4] step 500 loss 0.0423 lr 0.000008
[Epoch 4] step 600 loss 0.0767 lr 0.000008
[Epoch 4] step 700 loss 0.0623 lr 0.000008
[Epoch 4] step 800 loss 0.0719 lr 0.000008
[Epoch 4] step 900 loss 0.0280 lr 0.000008
[Epoch 4] step 1000 loss 0.0263 lr 0.000008
[Epoch 4] step 1100 loss 0.0898 lr 0.000008
[Epoch 4] step 1200 loss 0.0222 lr 0.000008
[Epoch 4] step 1300 loss 0.1246 lr 0.000008
[Epoch 4] step 1400 loss 0.0851 lr 0.000008
[Epoch 4] step 1500 loss 0.0594 lr 0.000008
[Epoch 4] step 1600 loss 0.0838 lr 0.000008
[Epoch 4] step 1700 loss 0.0504 lr 0.000008
[Epoch 4] step 1800 loss 0.0130 lr 0.000008
[Epoch 4] step 1900 loss 0.0328 lr 0.000008
[Epoch 4] step 2000 loss 0.0442 lr 0.000008
[Epoch 4] step 2100 loss 0.0456 lr 0.000008
[Epoch 4] step 2200 loss 0.1256 lr 0.000008
[Epoch 4] step 2300 loss 0.0207 lr 0.000008
[Epoch 4] step 2400 loss 0.0308 lr 0.000008
[Epoch 4] step 2500 loss 0.1099 lr 0.000008
[Epoch 4] step 2600 loss 0.0274 lr 0.000008
[Epoch 4] step 2700 loss 0.0724 lr 0.000008
[Epoch 4] step 2800 loss 0.0494 lr 0.000008
[Epoch 4] step 2900 loss 0.0322 lr 0.000008
[Epoch 4] step 3000 loss 0.0556 lr 0.000008
[Epoch 4] step 3100 loss 0.1156 lr 0.000008
[Epoch 4] step 3200 loss 0.0697 lr 0.000008
[Epoch 4] step 3300 loss 0.0571 lr 0.000008
[Epoch 4] step 3400 loss 0.0806 lr 0.000008
[Epoch 4] step 3500 loss 0.0491 lr 0.000008
[Epoch 4] step 3600 loss 0.0299 lr 0.000008
[Epoch 4] step 3700 loss 0.0581 lr 0.000008
[Epoch 4] step 3800 loss 0.0274 lr 0.000007
[Epoch 4] step 3900 loss 0.0243 lr 0.000007
[Epoch 4] step 4000 loss 0.0760 lr 0.000007
[Epoch 4] step 4100 loss 0.0569 lr 0.000007
[Epoch 4] step 4200 loss 0.0391 lr 0.000007
[Epoch 4] step 4300 loss 0.0781 lr 0.000007
[Epoch 4] step 4400 loss 0.0238 lr 0.000007
[Epoch 4] step 4500 loss 0.0756 lr 0.000007
[Epoch 4] step 4600 loss 0.1208 lr 0.000007
[Epoch 4] step 4700 loss 0.0152 lr 0.000007
[Epoch 4] step 4800 loss 0.1260 lr 0.000007
[Epoch 4] step 4900 loss 0.0687 lr 0.000007
[Validation] global_step 50000 loss 0.2463 accuracy 0.4727
Validation metrics did not improve. Patience: 4/10
[Epoch 4] step 5000 loss 0.0751 lr 0.000007
[Epoch 4] step 5100 loss 0.0553 lr 0.000007
[Epoch 4] step 5200 loss 0.0131 lr 0.000007
[Epoch 4] step 5300 loss 0.0962 lr 0.000007
[Epoch 4] step 5400 loss 0.0476 lr 0.000007
[Epoch 4] step 5500 loss 0.0424 lr 0.000007
[Epoch 4] step 5600 loss 0.0540 lr 0.000007
[Epoch 4] step 5700 loss 0.1739 lr 0.000007
[Epoch 4] step 5800 loss 0.0859 lr 0.000007
[Epoch 4] step 5900 loss 0.1150 lr 0.000007
[Epoch 4] step 6000 loss 0.0321 lr 0.000007
[Epoch 4] step 6100 loss 0.0649 lr 0.000007
[Epoch 4] step 6200 loss 0.0252 lr 0.000007
[Epoch 4] step 6300 loss 0.0517 lr 0.000007
[Epoch 4] step 6400 loss 0.0351 lr 0.000007
[Epoch 4] step 6500 loss 0.0546 lr 0.000007
[Epoch 4] step 6600 loss 0.1056 lr 0.000007
[Epoch 4] step 6700 loss 0.1116 lr 0.000007
[Epoch 4] step 6800 loss 0.0930 lr 0.000007
[Epoch 4] step 6900 loss 0.0333 lr 0.000007
[Epoch 4] step 7000 loss 0.0567 lr 0.000007
[Epoch 4] step 7100 loss 0.0370 lr 0.000007
[Epoch 4] step 7200 loss 0.0994 lr 0.000007
[Epoch 4] step 7300 loss 0.0409 lr 0.000007
[Epoch 4] step 7400 loss 0.0322 lr 0.000007
[Epoch 4] step 7500 loss 0.0292 lr 0.000007
[Epoch 4] step 7600 loss 0.0260 lr 0.000007
[Epoch 4] step 7700 loss 0.1009 lr 0.000007
[Epoch 4] step 7800 loss 0.0634 lr 0.000007
[Epoch 4] step 7900 loss 0.0610 lr 0.000007
[Epoch 4] step 8000 loss 0.0554 lr 0.000007
[Epoch 4] step 8100 loss 0.0407 lr 0.000007
[Epoch 4] step 8200 loss 0.0806 lr 0.000007
[Epoch 4] step 8300 loss 0.0734 lr 0.000007
[Epoch 4] step 8400 loss 0.0772 lr 0.000007
[Epoch 4] step 8500 loss 0.0395 lr 0.000007
[Epoch 4] step 8600 loss 0.2301 lr 0.000007
[Epoch 4] step 8700 loss 0.1392 lr 0.000007
[Epoch 4] step 8800 loss 0.1256 lr 0.000007
[Epoch 4] step 8900 loss 0.0567 lr 0.000007
[Epoch 4] step 9000 loss 0.0542 lr 0.000007
[Epoch 4] step 9100 loss 0.0682 lr 0.000007
[Epoch 4] step 9200 loss 0.0536 lr 0.000007
[Epoch 4] step 9300 loss 0.0559 lr 0.000007
[Epoch 4] step 9400 loss 0.0730 lr 0.000007
[Epoch 4] step 9500 loss 0.1106 lr 0.000007
[Epoch 4] step 9600 loss 0.0586 lr 0.000007
[Epoch 4] step 9700 loss 0.0413 lr 0.000007
[Epoch 4] step 9800 loss 0.1251 lr 0.000007
[Epoch 4] step 9900 loss 0.0743 lr 0.000007
[Validation] global_step 55000 loss 0.3534 accuracy 0.4805
Validation metrics did not improve. Patience: 5/10
[Epoch 4] step 10000 loss 0.0924 lr 0.000007
[Epoch 4] step 10100 loss 0.0698 lr 0.000007
[Epoch 4] step 10200 loss 0.1056 lr 0.000007
[Epoch 4] step 10300 loss 0.0926 lr 0.000007
[Epoch 4] step 10400 loss 0.1400 lr 0.000007
[Epoch 4] step 10500 loss 0.0834 lr 0.000007
[Epoch 4] step 10600 loss 0.0988 lr 0.000007
[Epoch 4] step 10700 loss 0.1015 lr 0.000007
[Epoch 4] step 10800 loss 0.0980 lr 0.000007
[Epoch 4] step 10900 loss 0.1512 lr 0.000007
[Epoch 4] step 11000 loss 0.0960 lr 0.000007
[Epoch 4] step 11100 loss 0.1283 lr 0.000007
[Epoch 4] step 11200 loss 0.1184 lr 0.000007
[Epoch 4] step 11300 loss 0.1410 lr 0.000007
[Epoch 4] step 11400 loss 0.1319 lr 0.000007
[Epoch 4] step 11500 loss 0.1412 lr 0.000007
[Epoch 4] step 11600 loss 0.1753 lr 0.000007
[Epoch 4] step 11700 loss 0.2517 lr 0.000007
[Epoch 4] step 11800 loss 0.2898 lr 0.000007
[Epoch 4] step 11900 loss 0.2652 lr 0.000007
[Epoch 4] step 12000 loss 0.2563 lr 0.000007
[Epoch 4] step 12100 loss 0.2892 lr 0.000007
[Epoch 4] step 12200 loss 0.3519 lr 0.000007
[Epoch 4] step 12300 loss 0.2968 lr 0.000007
[Epoch 4] step 12400 loss 0.3822 lr 0.000007
[Epoch 4] step 12500 loss 0.3332 lr 0.000007
[Epoch 4] step 12600 loss 0.4215 lr 0.000007
[Epoch 4] step 12700 loss 0.4623 lr 0.000007
[Epoch 4] step 12800 loss 0.4823 lr 0.000007
[Epoch 4] step 12900 loss 0.5216 lr 0.000007
[Epoch 4] step 13000 loss 0.5183 lr 0.000007
[Epoch 4] step 13100 loss 0.4925 lr 0.000007
[Epoch 4] step 13200 loss 0.5064 lr 0.000007
[Epoch 4] step 13300 loss 0.5497 lr 0.000007
[Epoch 4] step 13400 loss 0.5155 lr 0.000007
[Epoch 4] step 13500 loss nan lr 0.000007
[Epoch 4] step 13600 loss nan lr 0.000007
[Epoch 4] step 13700 loss nan lr 0.000007
[Epoch 4] step 13800 loss nan lr 0.000007
[Epoch 4] step 13900 loss nan lr 0.000007
[Epoch 4] step 14000 loss nan lr 0.000007
[Epoch 4] step 14100 loss nan lr 0.000007
[Epoch 4] step 14200 loss nan lr 0.000007
[Epoch 4] step 14300 loss nan lr 0.000007
[Epoch 4] step 14400 loss nan lr 0.000007
[Epoch 4] step 14500 loss nan lr 0.000007
[Epoch 4] step 14600 loss nan lr 0.000007
[Epoch 4] step 14700 loss nan lr 0.000007
[Epoch 4] step 14800 loss nan lr 0.000007
[Epoch 4] step 14900 loss nan lr 0.000007
